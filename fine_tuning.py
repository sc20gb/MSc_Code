import torchvision.transforms as transforms
import os
from data_loading import load_data,  display_sample
import torch

import numpy as np

from CLIP import CLIP, convert_weights

from transformers import LlamaForCausalLM, AutoTokenizer, get_cosine_schedule_with_warmup
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


from connector_LLM import Connector_LLM


import torch.nn.functional as F

from torcheval.metrics.functional import bleu_score

import csv

import wandb

#Return  the object that is the visual encoder, return the heat scaling parameter as well
def load_ViT_img_encoder(tokenizer,transformer_width,MAX_LENGTH,transformer_layers,transformer_heads,embed_dim,vision_width,image_resolution,vision_patch_size,vision_layers,device,clip_model_path):
    clip = CLIP(vocab_size=tokenizer.vocab_size, transformer_width=transformer_width,context_length=MAX_LENGTH,transformer_layers=transformer_layers,transformer_heads=transformer_heads, embed_dim=embed_dim, vision_width=vision_width, image_resolution=image_resolution, vision_patch_size=vision_patch_size, vision_layers=vision_layers,device=device)
    clip.load_state_dict(torch.load(clip_model_path))

    return clip.visual, clip.logit_scale

def calc_loss_and_metrics(predicted,target,tokenizer,max_length):

    print("Loss Metrics Calc:")

    # We need it to be a list of tensors instead 
    # Pad the predicted tensors after the </s> to the unk token [....,answer,2,44235,3153,...] -> [answer,2]
    predicted_text  = []
    for i in range(predicted.size(0)):
        # Find the index of the first occurrence of 2 in the current batch
        index_of_two = (predicted[i,max_length -  1:] == 2).nonzero(as_tuple=True)[0] if (predicted[i,max_length -  1:] == 2).any() else None
        if index_of_two is not None:
            # Replace all preceding values with unk_token
            #predicted[i, index_of_two[0]+ 1:] = tokenizer.unk_token_id
            predicted_text.append(predicted[i,max_length - 1:index_of_two[0] + 1].flatten())
        else:
            predicted_text.append(predicted[i,max_length - 1:].flatten())

    print("Predicted:")
    # Every value after the max length will be one that is generated by the LLM. Now both will be of isolated length so f1_score can be used
    print(predicted_text)

    print("Target:")
    print(target)

    # Calc accuracy
    accuracy = 0
    for i in range(len(target)):
            # This here is the same as EM see the link below
            if predicted_text[i].size(0) == target[i].size(0):
                accuracy += (predicted_text[i] == target[i]).all()

    print("Accuracy:")
    print(accuracy)

    # this score is calculated from the plain english sentences
    bleu_score_ = bleu_score(
        tokenizer.batch_decode(torch.stack(predicted_text),add_special_tokens=True),
        tokenizer.batch_decode(torch.stack(target),add_special_tokens=True),
        n_gram=1)

    print("Bleu_score:")
    print(bleu_score_)

    # https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#F1
    # precision here is the number of shared words / len(predict)
    #recall is the number of shared words / len(target)

    prec = 0.0
    rec = 0.0

    for i in range(len(target)):
            common_tokens = set(predicted_text[i]) & set(target[i])
            prec = len(common_tokens) / predicted_text[i].size(0)
            rec = len(common_tokens) /  target[i].size(0)
            print(predicted_text[i])
            print(target[i])
            print(common_tokens)

    if prec + rec == 0.0:
        f1 = 0.0
    else:
        f1 = 2 * (prec * rec) / (prec + rec)

    print("Prec:")
    print(prec)
    print("Rec:")
    print(rec)
    print("F1:")
    print(f1)

    return accuracy, bleu_score_,prec,rec,f1

# This trains the MLP between the visual encoder and LLM. It can be seen as traing a compatible visual tokenizer for the for the frosen LLM
def feature_aliginment_training_step_1(clip_parameters,optim_parameters,connector_llm_parameters,per_warm,image_size,batch_size,rand_seed,MAX_EPOC,MAX_LENGTH,VERSION,save=False):
    # CHECK GPU SUPPORT AND ASSIGN DEVICE
    if torch.cuda.is_available():
        # Get the number of GPUs available
        gpu_count = torch.cuda.device_count()
        print(f"CUDA is available with {gpu_count} GPU(s)!")

        # Print the name of each GPU available
        for i in range(gpu_count):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")

        device = torch.device("cpu")
    else:
        print("CUDA is not available. Training will proceed on CPU.")
        device = torch.device("cpu")

    #LOAD DATA
    test_loader, train_loader, validate_loader = load_data(transforms.Compose([
                                                           transforms.Resize((image_size, image_size)),
                                                           transforms.ToTensor()
                                                                              ]), batch_size, rand_seed, os.path.join(os.getcwd(), 'Slake1.0')
    )


    # Load connector and vicuna model

    connector_llm = Connector_LLM(**connector_llm_parameters,device=device,MAX_LENGTH=MAX_LENGTH)


    # FREEZE vicuna TRANING (should save  memory and computation as well)
    connector_llm.vicuna.eval()

    # LOAD ViT encoder from the CLIP model
    img_encoder,logit_scale = load_ViT_img_encoder(**clip_parameters,device=device,tokenizer=connector_llm.tokenizer,MAX_LENGTH=MAX_LENGTH)

    img_encoder = img_encoder.to(device)

    # FREEZE CLIP TRANING (should save  memory and computation as well)
    img_encoder.eval()

    #Optimizer and learning rate scheduling
    optim = torch.optim.AdamW(connector_llm.connector.parameters(), **optim_parameters)
    scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=MAX_EPOC // (100 * per_warm),num_training_steps=MAX_EPOC)

    # Record the loss at the epoch
    loss_epoch = []
    for n in range(1,MAX_EPOC + 1):
        connector_llm.connector.train()
        trainng_loss_avg  = torch.tensor([0.0])
        train_accuracy_avg = 0.0
        train_precision_avg = 0.0
        train_recall_avg = 0.0
        train_f1_avg = 0.0
        train_bleu_score_avg = 0.0
        count_t = 0
        count_q = 0
        for image_tensor, mask_tensor, question, answer in train_loader:

            optim.zero_grad()
            
            #Get image features from the img encoder
            image_features = img_encoder(image_tensor.to(device))
            
            #Format data and "tokenize" inputs for the LLM, combine them in the form <s> image_encoder_tokenized question </s>
            answer_ = [connector_llm.tokenizer(a + "</s>",return_tensors="pt", max_length = MAX_LENGTH).input_ids for a in answer]
            
            for i, a in enumerate(answer_):
                if len(a) < MAX_LENGTH:
                    answer_[i] = F.pad(a, (0, MAX_LENGTH - a.size(0)), 'constant', 0)

            #answer_ = torch.cat(answer_,0)[:, 1:].to(device)

            answer_ = torch.cat(answer_,dim=0)[:,1:].to(device)

            output, loss = connector_llm(image_features,question,answer_,15)

            accuracy,bleu_score,precision,recall,f1 = calc_loss_and_metrics(
                output,
                [connector_llm.tokenizer(a + "</s>",return_tensors="pt").input_ids[:,1:].flatten() for a in answer],
                tokenizer=connector_llm.tokenizer,
                max_length=MAX_LENGTH)
            
            loss.backward()

            optim.step()

            connector_llm.connector.zero_grad()

            trainng_loss_avg += loss.to('cpu')

            train_accuracy_avg += accuracy
            train_precision_avg += precision
            train_recall_avg += recall
            train_f1_avg += f1
            train_bleu_score_avg += bleu_score
            count_t +=1
            count_q += answer_.size(0)


        scheduler.step()

        #  VALIDTATE
        validation_loss_avg  = torch.tensor([0.0])
        val_accuracy_avg = 0.0
        val_precision_avg = 0.0
        val_recall_avg = 0.0
        val_f1_avg = 0.0
        val_count_q = 0.0
        val_bleu_score_avg = 0.0
        count=0
        connector_llm.connector.eval()
        with torch.no_grad():
             for image_tensor, mask_tensor, question, answer in validate_loader:
              #Get image features from the img encoder
                image_features = img_encoder(image_tensor.to(device))
                
                #Format data and "tokenize" inputs for the LLM, combine them in the form <s> image_encoder_tokenized question </s>
                answer_ = [connector_llm.tokenizer(a + "</s>",return_tensors="pt", max_length = MAX_LENGTH).input_ids for a in answer]
                
                for i, a in enumerate(answer_):
                    if len(a) < MAX_LENGTH:
                        answer_[i] = F.pad(a, (0, MAX_LENGTH - a.size(0)), 'constant', 0)

                #answer_ = torch.cat(answer_,0)[:, 1:].to(device)

                answer_ = torch.cat(answer_,dim=0)[:,1:].to(device)

                output, loss = connector_llm(image_features,question,answer_,5)

                accuracy,bleu_score,precision,recall,f1 = calc_loss_and_metrics(
                    output,
                    [connector_llm.tokenizer(a + "</s>",return_tensors="pt").input_ids[:,1:].flatten() for a in answer],
                    tokenizer=connector_llm.tokenizer,
                    max_length=MAX_LENGTH)
                trainng_loss_avg += loss.to('cpu')

                val_accuracy_avg += accuracy
                val_precision_avg += precision
                val_recall_avg += recall
                val_f1_avg += f1
                val_bleu_score_avg += bleu_score
                count +=1
                val_count_q += answer_.size(0)

            #SAVE RESULTS
        if save:
                if not os.path.exists(os.path.join("nobackup","sc20gwb","Models","SavedModels", "C_V_" + str(VERSION))):
                    os.makedirs(os.path.join(os.getcwd(),"SavedModels", "C_V_" + str(VERSION)))
                torch.save(connector_llm.connector.state_dict,os.path.join("nobackup","sc20gwb","Models","SavedModels", "C_V_" + str(VERSION),"connector_LLM_model" + str(n) + ".pth"))
            
        wandb.log({
                "loss_validate":validation_loss_avg.to('cpu').detach().numpy()[0]/count,
                "loss_training":trainng_loss_avg.to('cpu').detach().numpy()[0]/count_t,
                "val_accuracy_avg":val_accuracy_avg/val_count_q,
                "train_accuracy_avg":train_accuracy_avg/count_q,
                "val_precision_avg":val_precision_avg/count,
                "train_precision_avg":train_precision_avg/count_t,
                "val_recall_avg":val_recall_avg/count,
                "train_recall_avg":train_recall_avg/count_t,
                "val_f1_avg":val_f1_avg/count,
                "train_f1_avg":train_f1_avg/count_t,
                "train_bleu_score_avg": train_bleu_score_avg/count_t,
                "val_bleu_score_avg": val_bleu_score_avg/count
                  })


    return loss_epoch

def feature_aliginment_training_step_2(clip_parameters,optim_parameters,connector_llm_parameters,per_warm,image_size,batch_size,rand_seed,MAX_EPOC,MAX_LENGTH,VERSION,save=False):
    # CHECK GPU SUPPORT AND ASSIGN DEVICE
    if torch.cuda.is_available():
        # Get the number of GPUs available
        gpu_count = torch.cuda.device_count()
        print(f"CUDA is available with {gpu_count} GPU(s)!")

        # Print the name of each GPU available
        for i in range(gpu_count):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")

        device = torch.device("cpu")
    else:
        print("CUDA is not available. Training will proceed on CPU.")
        device = torch.device("cpu")

    #LOAD DATA
    test_loader, train_loader, validate_loader = load_data(transforms.Compose([
                                                           transforms.Resize((image_size, image_size)),
                                                           transforms.ToTensor()
                                                                              ]), batch_size, rand_seed, os.path.join(os.getcwd(), 'Slake1.0')
    )


    # Load connector and vicuna model

    connector_llm = Connector_LLM(**connector_llm_parameters,device=device,MAX_LENGTH=MAX_LENGTH)

    # LOAD ViT encoder from the CLIP model
    img_encoder,logit_scale = load_ViT_img_encoder(**clip_parameters,device=device,tokenizer=connector_llm.tokenizer,MAX_LENGTH=MAX_LENGTH)

    img_encoder = img_encoder.to(device)

    # FREEZE CLIP TRANING (should save  memory and computation as well)
    img_encoder.eval()

    #Optimizer and learning rate scheduling
    optim = torch.optim.AdamW(connector_llm.connector.parameters(), **optim_parameters)
    scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=MAX_EPOC // (100 * per_warm),num_training_steps=MAX_EPOC)

    # Record the loss at the epoch
    loss_epoch = []
    for n in range(1,MAX_EPOC + 1):
        connector_llm.train()
        trainng_loss_avg  = torch.tensor([0.0])
        train_accuracy_avg = 0.0
        train_precision_avg = 0.0
        train_recall_avg = 0.0
        train_f1_avg = 0.0
        train_bleu_score_avg = 0.0
        count_t = 0
        count_q = 0
        for image_tensor, mask_tensor, question, answer in train_loader:

            optim.zero_grad()
            
            #Get image features from the img encoder
            image_features = img_encoder(image_tensor.to(device))
            
            #Format data and "tokenize" inputs for the LLM, combine them in the form <s> image_encoder_tokenized question </s>
            answer_ = [connector_llm.tokenizer(a + "</s>",return_tensors="pt", max_length = MAX_LENGTH).input_ids for a in answer]
            
            for i, a in enumerate(answer_):
                if len(a) < MAX_LENGTH:
                    answer_[i] = F.pad(a, (0, MAX_LENGTH - a.size(0)), 'constant', 0)

            #answer_ = torch.cat(answer_,0)[:, 1:].to(device)

            answer_ = torch.cat(answer_,dim=0)[:,1:].to(device)

            output, loss = connector_llm(image_features,question,answer_,15)

            accuracy,bleu_score,precision,recall,f1 = calc_loss_and_metrics(
                output,
                [connector_llm.tokenizer(a + "</s>",return_tensors="pt").input_ids[:,1:].flatten() for a in answer],
                tokenizer=connector_llm.tokenizer,
                max_length=MAX_LENGTH)
            
            loss.backward()

            optim.step()

            connector_llm.zero_grad()

            trainng_loss_avg += loss.to('cpu')

            train_accuracy_avg += accuracy
            train_precision_avg += precision
            train_recall_avg += recall
            train_f1_avg += f1
            train_bleu_score_avg += bleu_score
            count_t +=1
            count_q += answer_.size(0)


        scheduler.step()

        #  VALIDTATE
        validation_loss_avg  = torch.tensor([0.0])
        val_accuracy_avg = 0.0
        val_precision_avg = 0.0
        val_recall_avg = 0.0
        val_f1_avg = 0.0
        val_count_q = 0.0
        val_bleu_score_avg = 0.0
        count=0
        connector_llm.eval()
        with torch.no_grad():
             for image_tensor, mask_tensor, question, answer in validate_loader:
              #Get image features from the img encoder
                image_features = img_encoder(image_tensor.to(device))
                
                #Format data and "tokenize" inputs for the LLM, combine them in the form <s> image_encoder_tokenized question </s>
                answer_ = [connector_llm.tokenizer(a + "</s>",return_tensors="pt", max_length = MAX_LENGTH).input_ids for a in answer]
                
                for i, a in enumerate(answer_):
                    if len(a) < MAX_LENGTH:
                        answer_[i] = F.pad(a, (0, MAX_LENGTH - a.size(0)), 'constant', 0)

                answer_ = torch.cat(answer_,dim=0)[:,1:].to(device)

                output, loss = connector_llm(image_features,question,answer_,5)

                accuracy,bleu_score,precision,recall,f1 = calc_loss_and_metrics(
                    output,
                    [connector_llm.tokenizer(a + "</s>",return_tensors="pt").input_ids[:,1:].flatten() for a in answer],
                    tokenizer=connector_llm.tokenizer,
                    max_length=MAX_LENGTH)
                trainng_loss_avg += loss.to('cpu')

                val_accuracy_avg += accuracy
                val_precision_avg += precision
                val_recall_avg += recall
                val_f1_avg += f1
                val_bleu_score_avg += bleu_score
                count +=1
                val_count_q += answer_.size(0)

            #SAVE RESULTS
        if save:
                if not os.path.exists(os.path.join("nobackup","sc20gwb","Models","SavedModels", "C_V_" + str(VERSION))):
                    os.makedirs(os.path.join(os.getcwd(),"SavedModels", "C_V_" + str(VERSION)))
                torch.save(connector_llm.connector.state_dict,os.path.join("nobackup","sc20gwb","Models","SavedModels", "C_V_" + str(VERSION),"connector_LLM_model" + str(n) + ".pth"))
            
        wandb.log({
                "loss_validate":validation_loss_avg.to('cpu').detach().numpy()[0]/count,
                "loss_training":trainng_loss_avg.to('cpu').detach().numpy()[0]/count_t,
                "val_accuracy_avg":val_accuracy_avg/val_count_q,
                "train_accuracy_avg":train_accuracy_avg/count_q,
                "val_precision_avg":val_precision_avg/count,
                "train_precision_avg":train_precision_avg/count_t,
                "val_recall_avg":val_recall_avg/count,
                "train_recall_avg":train_recall_avg/count_t,
                "val_f1_avg":val_f1_avg/count,
                "train_f1_avg":train_f1_avg/count_t,
                "train_bleu_score_avg": train_bleu_score_avg/count_t,
                "val_bleu_score_avg": val_bleu_score_avg/count
                  })


    return loss_epoch

clip_parameters  =  {

"transformer_width":512,
"transformer_layers":12,
"transformer_heads":8,
"embed_dim":512,
"vision_width":768,
"image_resolution":224,
"vision_patch_size":8,
"vision_layers":12,
"clip_model_path": os.path.join(os.getcwd(), "Models_to_upload", "clip_model_45.pth")

}

optim_parameters = {

"lr":0.0001,

"eps":0.0001,

"weight_decay":0.0001

}

connector_llm_parameters = {
"vicuna_path":os.path.join("/nobackup","sc20gwb","Models", "vicuna-7b-v1.5"),
"embed_dim": 512,
"connector_width":512,
"connector_layers":2,
"connector_output":36
}

wandb.init(
        # set the wandb project where this run will be logged
        project="MSc",
        dir="/nobackup/sc20gwb",
        # track hyperparameters and run metadata
        config= {**connector_llm_parameters,**optim_parameters, **clip_parameters}
    )


feature_aliginment_training_step_1(clip_parameters,optim_parameters,connector_llm_parameters,per_warm=0.2,image_size=224,batch_size=2,rand_seed=42,MAX_EPOC=3,MAX_LENGTH=256,VERSION=2000, save=True)