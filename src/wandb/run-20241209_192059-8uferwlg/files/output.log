Only one GPU available, models are split between GPU 0
loading pretrained CLIP visual encoder
0  warmup steps
231  total steps
1229.5  train batches
C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\transformers\models\llama\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Traceback (most recent call last):
  File "c:\Users\George\Desktop\Repositories\MSc_Code\fine_tune_with_gen.py", line 501, in <module>
    feature_aliginment_training(**para)
  File "c:\Users\George\Desktop\Repositories\MSc_Code\fine_tune_with_gen.py", line 312, in feature_aliginment_training
    loss.backward()
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\autograd\__init__.py", line 267, in backward
    _engine_run_backward(
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\autograd\graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt