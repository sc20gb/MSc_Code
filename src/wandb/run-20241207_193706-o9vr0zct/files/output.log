Only one GPU available, models are split between GPU 0
loading pretrained CLIP visual encoder
0  warmup steps
770  total steps
1229.5  train batches
C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\transformers\models\llama\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
tensor(42.8067, device='cuda:0', grad_fn=<DivBackward0>)
tensor(41.3975, device='cuda:0', grad_fn=<DivBackward0>)
tensor(20.0631, device='cuda:0', grad_fn=<DivBackward0>)
tensor(108.2359, device='cuda:0', grad_fn=<DivBackward0>)
tensor(40.2540, device='cuda:0', grad_fn=<DivBackward0>)
tensor(39.6794, device='cuda:0', grad_fn=<DivBackward0>)
tensor(51.2940, device='cuda:0', grad_fn=<DivBackward0>)
tensor(29.8104, device='cuda:0', grad_fn=<DivBackward0>)
tensor(30.3659, device='cuda:0', grad_fn=<DivBackward0>)
tensor(195.0054, device='cuda:0', grad_fn=<DivBackward0>)
tensor(51.6650, device='cuda:0', grad_fn=<DivBackward0>)
tensor(41.9527, device='cuda:0', grad_fn=<DivBackward0>)
tensor(43.2225, device='cuda:0', grad_fn=<DivBackward0>)
tensor(61.3927, device='cuda:0', grad_fn=<DivBackward0>)
tensor(30.9443, device='cuda:0', grad_fn=<DivBackward0>)
tensor(51.1293, device='cuda:0', grad_fn=<DivBackward0>)
tensor(49.2693, device='cuda:0', grad_fn=<DivBackward0>)
tensor(29.5957, device='cuda:0', grad_fn=<DivBackward0>)
tensor(109.3084, device='cuda:0', grad_fn=<DivBackward0>)
tensor(20.0266, device='cuda:0', grad_fn=<DivBackward0>)
tensor(38.6987, device='cuda:0', grad_fn=<DivBackward0>)
tensor(38.2771, device='cuda:0', grad_fn=<DivBackward0>)
tensor(29.6597, device='cuda:0', grad_fn=<DivBackward0>)
tensor(40.2780, device='cuda:0', grad_fn=<DivBackward0>)
tensor(30.0652, device='cuda:0', grad_fn=<DivBackward0>)
tensor(41.4216, device='cuda:0', grad_fn=<DivBackward0>)
tensor(28.0920, device='cuda:0', grad_fn=<DivBackward0>)
tensor(89.2378, device='cuda:0', grad_fn=<DivBackward0>)
tensor(59.1580, device='cuda:0', grad_fn=<DivBackward0>)
tensor(42.5292, device='cuda:0', grad_fn=<DivBackward0>)
tensor(39.5398, device='cuda:0', grad_fn=<DivBackward0>)
tensor(43.2749, device='cuda:0', grad_fn=<DivBackward0>)
tensor(54.7112, device='cuda:0', grad_fn=<DivBackward0>)
tensor(41.7810, device='cuda:0', grad_fn=<DivBackward0>)
tensor(77.7085, device='cuda:0', grad_fn=<DivBackward0>)
tensor(30.5266, device='cuda:0', grad_fn=<DivBackward0>)
tensor(51.2526, device='cuda:0', grad_fn=<DivBackward0>)
tensor(91.8071, device='cuda:0', grad_fn=<DivBackward0>)
tensor(39.1608, device='cuda:0', grad_fn=<DivBackward0>)
tensor(52.3987, device='cuda:0', grad_fn=<DivBackward0>)
tensor(20.9843, device='cuda:0', grad_fn=<DivBackward0>)
tensor(20.6726, device='cuda:0', grad_fn=<DivBackward0>)
tensor(29.7720, device='cuda:0', grad_fn=<DivBackward0>)
tensor(71.6766, device='cuda:0', grad_fn=<DivBackward0>)
tensor(39.7639, device='cuda:0', grad_fn=<DivBackward0>)
tensor(63.7033, device='cuda:0', grad_fn=<DivBackward0>)
tensor(53.0022, device='cuda:0', grad_fn=<DivBackward0>)
tensor(48.0559, device='cuda:0', grad_fn=<DivBackward0>)
tensor(39.5534, device='cuda:0', grad_fn=<DivBackward0>)
tensor(70.9741, device='cuda:0', grad_fn=<DivBackward0>)
tensor(40.1709, device='cuda:0', grad_fn=<DivBackward0>)
tensor(49.3222, device='cuda:0', grad_fn=<DivBackward0>)
tensor(40.9052, device='cuda:0', grad_fn=<DivBackward0>)
tensor(51.5500, device='cuda:0', grad_fn=<DivBackward0>)
tensor(60.2820, device='cuda:0', grad_fn=<DivBackward0>)
tensor(49.4108, device='cuda:0', grad_fn=<DivBackward0>)
tensor(41.6436, device='cuda:0', grad_fn=<DivBackward0>)
tensor(30.4014, device='cuda:0', grad_fn=<DivBackward0>)
tensor(40.9559, device='cuda:0', grad_fn=<DivBackward0>)
tensor(30.4723, device='cuda:0', grad_fn=<DivBackward0>)
tensor(30.9416, device='cuda:0', grad_fn=<DivBackward0>)
tensor(52.9849, device='cuda:0', grad_fn=<DivBackward0>)
tensor(30.7663, device='cuda:0', grad_fn=<DivBackward0>)
tensor(168.5304, device='cuda:0', grad_fn=<DivBackward0>)
tensor(39.2852, device='cuda:0', grad_fn=<DivBackward0>)
Traceback (most recent call last):
  File "C:\Users\George\Desktop\Repositories\MSc_Code\fine_tune_with_gen.py", line 505, in <module>
    feature_aliginment_training(**para)
  File "C:\Users\George\Desktop\Repositories\MSc_Code\fine_tune_with_gen.py", line 291, in feature_aliginment_training
    _, hidden_states = img_encoder(image_tensor.to(device_image_encoder),return_hidden_states=True)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\Desktop\Repositories\MSc_Code\Model_Defs\CLIP_with_LORA.py", line 13, in forward
    vision_outputs = self.visual_encoder(x, output_hidden_states=return_hidden_states)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\transformers\models\clip\modeling_clip.py", line 1097, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\transformers\models\clip\modeling_clip.py", line 877, in forward
    layer_outputs = encoder_layer(
                    ^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\transformers\models\clip\modeling_clip.py", line 618, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\transformers\models\clip\modeling_clip.py", line 573, in forward
    hidden_states = self.fc1(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\George\anaconda3\envs\CondaforDeepLearning\Lib\site-packages\torch\nn\modules\linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt