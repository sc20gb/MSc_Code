## Project Title:
Medical Image Understanding through Vision-Language Models
## Aim:
Create a Multi-Model Large Language Model (MLLM) for a range of radiology tasks; diagnostic aid, report generation, and teaching capabilities
## Objective:
Create an MLLM that:

Specific: Contains a visual encoder capable of capturing fine-grained positional details in medical images to be integrated with pre-trained Large Language Models

Measurable: Can gain competitive or SOTA performance in medical VQA and report generation tasks

Achievable: Utilizes pre-trained visual encoders fine-tuned on medical data

Relevant: Address the visual encoding bottleneck in MLLMs by providing higher-quality embeddings to language generation tasks

Time-Bound: Complete the project by the PhD deadline: 30th September 2027


## The project intends to answer the following:

Can the visual encoder bottleneck be addressed in MLLMs by capturing fine-level detail?

Can we effectively evaluate VQA and report generation?

How can generalisability be produced across different datasets/medical domains?

What work on MLLMs has been applied in the medical domain and how have they dealt with the smaller amounts of data?

Can the projection of visual embeddings to the language models embedding space be improved?

