# Week Progress

**Date**: 2024-11-25 to 2024-11-29

## Overview
Sevral updates were made to the MLLM class made by modifying the transformers library. The system does not work on the GPU if the CLIP model is running on the same device. After ensuring that the two models were syncronized the problem persisted. A solution was found by running the CLIP model on the cpu. However this must slow down training so must be resolved.

After running several models with no warmup or scheduler we found the intial results of the new MLLM class with TinyLLama improved on the performance of the baseline model created with the Vicuna-7b LLM:

![Alt text](Images/W&B%20Chart%2029_11_2024,%2011_41_44.png)
![Alt text](Images/W&B%20Chart%2029_11_2024,%2011_41_58.png)
![Alt text](Images/W&B%20Chart%2029_11_2024,%2011_42_03.png)
![Alt text](Images/W&B%20Chart%2029_11_2024,%2011_42_08.png)



<div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap;">
  <img src="Images/W&B%20Chart%2029_11_2024,%2011_42_08.png" alt="Image 1" style="max-width: 45%; height: auto; margin: 10px;">
  <img src="Images/W&B%20Chart%2029_11_2024,%2011_42_08.png" alt="Image 2" style="max-width: 45%; height: auto; margin: 10px;">
</div>



Files modified:


## Accomplishments

## Plans for Next Week

- Increase the speed of the model by:
    - Getting both models to run on the same device
    - Moving the gentation of the loss to outside of the new MLLM and LlamaForCausalLMCustom classes
    - Compare the use of a scheduling and warmup on the models performance 

- Get some results/ a working model with the new MLLM model

- Catchup on paper reading