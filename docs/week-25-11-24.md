# Week Progress

**Date**: 2024-11-25 to 2024-11-29

## Overview
Sevral updates were made to the MLLM class made by modifying the transformers library. The system does not work on the GPU if the CLIP model is running on the same device. After ensuring that the two models were syncronized the problem persisted. A solution was found by running the CLIP model on the cpu. However this must slow down training so must be resolved.

After running several models with no warmup or scheduler we found the intial results of the new MLLM class with TinyLLama improved on the performance of the baseline model created with the Vicuna-7b LLM:

# Accuracy:
<div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap;">
  <img src="Images/W&B%20Chart%2029_11_2024,%2011_42_03.png" alt="Image 1" style="max-width: 50%; height: auto; margin: 10px;">
  <img src="Images/W&B%20Chart%2029_11_2024,%2011_42_08.png" alt="Image 2" style="max-width: 50%; height: auto; margin: 10px;">
</div>

# Loss:
<div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap;">
  <img src="Images/W&B%20Chart%2029_11_2024,%2011_41_44.png" alt="Image 1" style="max-width: 50%; height: auto; margin: 10px;">
  <img src="Images/W&B%20Chart%2029_11_2024,%2011_41_58.png" alt="Image 2" style="max-width: 50%; height: auto; margin: 10px;">
</div>

Files modified:

[fine tuning script](https://github.com/sc20gb/MSc_Code/blob/main/fine_tune_with_gen.py)

[MLLM class](https://github.com/sc20gb/MSc_Code/blob/main/Model_Defs%2Fconnector_LLM_with_gen.py)


## Accomplishments

- Improved the performance of th model significantly, baseline validation performance was 24.12%
- Refined the new MLLM class
- Idntified new ways to imrove the efficency of the model further


## Plans for Next Week

- Increase the speed of the model by:
    - Getting both models to run on the same device
    - Moving the gentation of the loss to outside of the new MLLM and LlamaForCausalLMCustom classes
- Compare the use of a scheduling and warmup on the models performance 
- Evaluate the best performing model on test data
- Catchup on paper reading