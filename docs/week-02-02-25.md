# Week Progress

**Date**: 2025-02-03 to 2025-02-07

## Overview

- DeepSeek was explored further
- Medical papers from the litriture review from week 20-01-25 have been explored further to:
    - Assess if they use a pre-trained vison encoder and LLM connected through a type of projection module
    - And if they have to see how they performed their embedding alignment

## DeepSeek-V3
<img src="Images/DeepSeekPNG.png" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">

- Mixture-of-Experts (MoE): 671B total parameters, but only 37B active per token.

- Expert Selection: Uses 8 out of 256 experts plus shared experts.

- Gating Network: Dynamically routes inputs to the most relevant experts:

\begin{align}
h'_t &= u_t + \sum_{i=1}^{N_s} \text{FFN}^{(s)}_i (u_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}^{(r)}_i (u_t), \tag{12} \\
g_{i,t} &= \frac{g'_{i,t}}{\sum_{j=1}^{N_r} g'_{j,t}}, \tag{13} \\
g'_{i,t} &=
\begin{cases}
    s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t} \mid 1 \leq j \leq N_r\}, K_r) \\
    0, & \text{otherwise},
\end{cases} \tag{14} \\
s_{i,t} &= \sigma (u_t^T e_i), \tag{15}
\end{align}

- Multi-Head Latent Attention (MLA): Reduces memory usage while maintaining efficiency

## Embedding Alignment in Medial MLLMs

## Accomplishments

## Plans for Next Week
