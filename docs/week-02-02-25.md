# Week Progress

**Date**: 2025-02-03 to 2025-02-07

## Overview

- DeepSeek was explored further
- Medical papers from the litriture review from week 20-01-25 have been explored further to:
    - Assess if they use a pre-trained vison encoder and LLM connected through a type of projection module
    - And if they have to see how they performed their embedding alignment

## DeepSeek-V3
<img src="Images/DeepSeekPNG.png" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">

- Mixture-of-Experts (MoE): 671B total parameters, but only 37B active per token.

- Expert Selection: Uses 8 out of 256 experts plus shared experts.

- Gating Network: Dynamically routes inputs to the most relevant experts:

<img src="Images/MoEeq.png" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">

- Multi-Head Latent Attention (MLA): Reduces memory usage while maintaining efficiency

## Embedding Alignment in Medial MLLMs

## Accomplishments

## Plans for Next Week
