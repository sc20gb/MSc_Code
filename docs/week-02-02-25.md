# Week Progress

**Date**: 2025-02-03 to 2025-02-07

## Overview

- DeepSeek was explored further
- Medical papers from the litriture review from week 20-01-25 have been explored further to:
    - Assess if they use a pre-trained vison encoder and LLM connected through a type of projection module
    - And if they have to see how they performed their embedding alignment

## DeepSeek-V3
![DeepSeek Architecture](Images/DeepSeekPNG.png)

- Mixture-of-Experts (MoE): 671B total parameters, but only 37B active per token.

- Expert Selection: Uses 8 out of 256 experts plus shared experts.

- Gating Network: Dynamically routes inputs to the most relevant experts:

![MoE Equations](Images/MoEeq.png)

- Multi-Head Latent Attention (MLA): Reduces memory usage while maintaining efficiency

## Embedding Alignment in Medial MLLMs

## Accomplishments

## Plans for Next Week
