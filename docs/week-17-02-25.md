# Week Progress

**Date**: 2025-02-17 to 2025-02-21

## Overview

### Code
The 3-stage program training implementation is now running on Aire. Some issues with the dataset were noted as the scale of images increased from 10,000 to 30,000, which has been resolved. The training of the MLP projection layer is now far more stable after removing a legacy coding mistake in the connector.

### Mysterious Projections

I found a very relevant and interesting paper: [Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual
Capabilities Without Richer Cross-Modal Projections](https://arxiv.org/pdf/2402.16832v1). The paper investigates how LLaVA gains domain-specific visual capabilities when fine-tuned. They question whether improvements in domain-specific understanding are due to the changes in the MLP projection or those within the LLM.

They train an independent MLP on the post-projection image representations to assess their domain-specific expressiveness.

The results show that post-projection representations are less expressive after fine-tuning, reinforcing the idea that the LLM, not the projection layer, is responsible for improved domain adaptation.

<img src="Images/MLP-results.png" alt="Image 1" style="flex: 1; max-width: 40%; height: auto;">

### Results so far

#### Stages 1-2-3
The validation loss for three stages of training (3 epochs of general data). Loss is decreasing with alignment tuning as expected:
<img src="Images/wandb/W&B Chart 21_02_2025, 14_13_30.png" alt="Image 1" style="flex: 1; max-width: 75%; height: auto;">

The accuracy for each stage increases, but not the first which is predicting longer captions:
<img src="Images/wandb/W&B Chart 21_02_2025, 14_15_21.png" alt="Image 1" style="flex: 1; max-width: 75%; height: auto;">

#### 1-2-3 vs 2-3 (with or without general data)

The training accuracy when using general data (step 1), furthers the tranning accuracy but not the validation:
<img src="Images/wandb/W&B Chart 21_02_2025, 14_16_01.png" alt="Image 1" style="flex: 1; max-width: 75%; height: auto;">

The validation loss of the alignment tuning stage on the SLAKE data (step 2) is lower and more stable given a prior stage 1 step.
<img src="Images/wandb/W&B Chart 21_02_2025, 14_18_07.png" alt="Image 1" style="flex: 1; max-width: 75%; height: auto;">

The Training accuracy after the second stage for 1-2 step training versus just step 2, when evaluated on SLAKE, is higher. This indicates that general data can be used to supplement or support a smaller domain data?

<img src="Images/wandb/W&B Chart 21_02_2025, 14_18_17.png" alt="Image 1" style="flex: 1; max-width: 75%; height: auto;">

### FFPR

The first draft of the [FFPR](https://www.overleaf.com/project/67a9c628ee5c48d784174552) is complete, minus the thesis plan.


## Accomplishments

- Completed the first draft of the first formal progress review
- Set up aire
- Solved the projection training instability
- Evaluated the performance of using general and domain specific data

## Plans for Next Week

- Run with Cross fold Val to ensure results
- Ablation study of method
