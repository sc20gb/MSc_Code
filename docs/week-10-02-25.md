# Week Progress

**Date**: 2025-02-10 to 2025-02-14

## Overview

- Refactored code to reduce the CLIP model bottleneck, so that the model does not slow down training. This was implemented through [pre_embedding_loading.py](https://github.com/sc20gb/MSc_Code/blob/main/src/Data_Loading/pre_embedding_loading.py), these classes allow for the evaluation of image embeddings prior to training.

- Implemented a dataset class for the laion coco dataset. Implemented in [data_loading.py](https://github.com/sc20gb/MSc_Code/blob/main/src/Data_Loading/data_loading.py) the efficient creation of the dataset from the provided URLs and caption data. Efficient loading was needed due to the memory constraints of such large datasets. This class can be changed to work for any image dataset that uses URLs in this way.

- Created a new training paradigm allowing for the specification of training stages in any combination/length of stages. This is implemented in [feature_aliginment_training](https://github.com/sc20gb/MSc_Code/blob/main/src/Training/feature_aliginment_training.py)

- This system was tested on a small subset of data.

- A start has been made for the [First Formal Progress Review](https://www.overleaf.com/project/67a9c628ee5c48d784174552) (FFPR) has been started


## Laion coco 600M dataset

- The paper for this dataset can be found here [Laion Coco 600M](https://laion.ai/blog/laion-coco/)

## Accomplishments

- Created the training pipeline for the new 3-stage training
- Made a start on the FFPR
- Gained access to Aire

## Plans for Next Week
- Use the pipeline to compare the training of different stage combinations
- Run this on Aire
- Work on FFPR
- AI^2


