# Week Progress

**Date**: 2025-01-20 to 2025-01-31

## Overview

- A literature review of MLLMs and LMMs was conducted
- The DeepSeek Architecture was explored

### Review

- From 284 papers
- 168 were irrelevant leaving 116 MLLM or LMM Models across a range of tasks and domains
- 25 were in some medical field
- Of the 116 architectures most followed the embedding projection format, while others used additional modules and methods to try and improve on SOTA.

<img src="Images/medicalvsnonmedical.PNG" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">


## Accomplishment

- Completed a base literature review of MLLMs and LMMs. Exploring the domains and modalities used.
- Explored the architecture of Deepseek-R1, in preparation to explore fine-tuning of latent attention and MoE LLMs.


## Plans for Next Week

- Review the methods fine-tuning MLLMs on specific domains
- Take a look at the Deepseek-R1 paper to explore potential fine-tuning in MLLMs
