# Week Progress

**Date**: 2025-03-10 to 2025-03-15


## Overview

The initial idea was to enforce the loss:

<div align="center">
   <p>cos(x, P<sub>1</sub>(x)) = (x · P<sub>1</sub>(x)) / (||x|| · ||P<sub>1</sub>(x)||)</p>
</div>

Where:

- <p>x is the original image encoding (from a vision encoder like CLIP, ViT, etc.)</p>
- <p>P<sub>1</sub>(x) is the projected embedding in the LLM token space</p>
- <p>The dot product x · P<sub>1</sub>(x) measures alignment between vectors</p>
- <p>||x|| and ||P<sub>1</sub>(x)|| are the L2 norms of the respective vectors</p>

This was meant to prevent excessive transformation: Ensures that the MLP doesn't distort features too far that they could not be recovered. Regularising the LLMs effect on selecting relevant information.
However, this ran into some inconsistent results, with only minor improvements.

The MLLM takes an image encoding and then projects it with an MLP to the LLM embedding space. We create a loss from the LLM token prediction loss. We add this loss to another, the cosine similarity between the projected image encoding and the projected image encoding projected again by another MLP projection.

<div align="center">
   <p>L<sub>cos</sub> = 1 - (P<sub>1</sub>(x) · P<sub>2</sub>(P<sub>1</sub>(x))) / (||P<sub>1</sub>(x)|| · ||P<sub>2</sub>(P<sub>1</sub>(x))||)</p>
</div>

The new approach avoids forcing the LLM space to match the vision space, which might have limited adaptation. Instead, it stabilizes the learned embeddings within the LLM's space, allowing them to be more structured and useful for token prediction. Which is critical for reasoning tasks like VQA.


# Regularising MLLM Alignment through Cosine Constrainment
<div align="center">
<p>L<sub>Cos</sub> = 1 - (x · P<sub>1</sub>(x)) / (||x|| · ||P<sub>1</sub>(x)||)</p>
</div>

<div align="center">
<p>L = L<sub>TP</sub> + &lambda;L<sub>Cos</sub>
</div>


<img src="Images/mistake/Cosine_Constrainment.PNG" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">


## Results


# Regularising MLLM Alignment through Self-Distillation
<div align="center">
    <p>L<sub>Cos</sub> = 1 - (P<sub>1</sub>(x) · P<sub>2</sub>(P<sub>1</sub>(x))) / (||P<sub>1</sub>(x)|| · ||P<sub>2</sub>(P<sub>1</sub>(x))||)</p>
</div>

<div align="center">
    <p>L = L<sub>TP</sub> + &lambda;L<sub>Cos</sub>
</div>


<img src="Images/mistake/self-dsistillation.PNG" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">

- <p>P<sub>1</sub>(x) the image embeddings projected to the LLM embedding space</p>
- <p>P<sub>2</sub>(P<sub>1</sub>(x)) the image embeddings in the LLM embedding space regularised</p>

## Strengthening Visual-Language Alignment

The model projects the image encoding into the LLM embedding space using an MLP projection layer. However, raw image encodings might not be perfectly aligned with the LLM token space. The additional cosine similarity loss forces the projected image embeddings to be more consistent and structured in the LLM's learned space. This means the projected image features become more compatible with text embeddings, making it easier for the LLM to use them for reasoning. This reduces the modality gap between vision and language, leading to better VQA performance.

## Regularization

- First projection: Image encoding projected into the LLM embedding space.
- Second projection: The already projected embedding again projected with another MLP.

By enforcing cosine similarity between these two representations, a form of regularization is introduced that stabilizes the learned image representations. This:

- Encourages consistency in the projected image representations.
- Prevents the MLP from mapping image features into a space that is too arbitrary.
- Forces the projections to preserve key semantic features across multiple transformations.

This prevents the model from overfitting to spurious correlations and encourages it to focus on salient visual features relevant to language-based reasoning.

## Inducing Better Image Representations for LLM Tokens

The primary LLM token prediction loss encourages image features to be useful for text-based reasoning. However, by adding the cosine similarity loss:

The projected visual embeddings remain structured in a way that preserves their original semantic meaning.
The model doesn't collapse into trivial solutions where the image embeddings lose their meaningful relationships when projected into text space.
This results in more accurate text predictions based on image inputs, which is crucial for VQA.


## Avoiding Information Degradation in Projection Layers

When using an MLP projection, there's a risk that the mapping loses information due to:

- Overfitting to training data (causing poor generalization).
- Collapsing embeddings to a low-rank subspace that lacks useful features (The LLM embedding space in this case is much larger 768 vs 2048).

By introducing the cosine similarity loss, you force the projected embeddings to retain structure, making it easier for the LLM to use the image information effectively.
## But why? Enforcing Representation Consistency

By adding a cosine similarity loss between <span>P<sub>1</sub>(x)</span> and <span>P<sub>2</sub>(P<sub>1</sub>(x))</span>, self-consistency is enforced in the learned embeddings.

This means that:
1. <span>P<sub>1</sub>(x)</span> represents the first projection from image encoding space to LLM embedding space
2. <span>P<sub>2</sub>(P<sub>1</sub>(x))</span> represents the second projection applied to the already projected embedding

The cosine similarity loss <span>L<sub>cos</sub> = 1 - (P<sub>1</sub>(x) · P<sub>2</sub>(P<sub>1</sub>(x))) / (||P<sub>1</sub>(x)|| · ||P<sub>2</sub>(P<sub>1</sub>(x))||)</span> ensures that the two projections maintain structural consistency.

When minimized, this loss encourages:
- <span>P<sub>1</sub>(x)</span> to produce embeddings that are robust to further transformation
- <span>P<sub>2</sub></span> to preserve the semantic structure of <span>P<sub>1</sub>(x)</span>
- The entire pipeline to learn stable, information-preserving projections
# Results

## Validation Accuracy

<img src="Images/mistake/W&B Chart 13_03_2025, 18_31_32.png" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">


## Embeddings consistency


## Other works

https://arxiv.org/pdf/2105.04906

https://arxiv.org/pdf/2407.04600v1

Other models preserve this consistency through attention mechanisms like flamingo using a gated cross-attention module to align visual tokens with LLM embeddings