# Week Progress

**Date**: 2025-03-10 to 2025-03-15


## Overview

The initial idea was to introduce the loss:

<div align="center">
    <p>L<sub>Cos</sub> = 1 - (x · P<sub>1</sub>(x)) / (||x|| · ||P<sub>1</sub>(x)||)</p>
</div>

Where:

- <p>x is the original image encoding (from a vision encoder like CLIP, ViT, etc.)</p>
- <p>P<sub>1</sub>(x) is the projected embedding in the LLM token space</p>
- <p>The dot product x · P<sub>1</sub>(x) measures alignment between vectors</p>
- <p>||x|| and ||P<sub>1</sub>(x)|| are the L2 norms of the respective vectors</p>

Ensures that the MLP doesn't distort features too far that they could not be recovered. Regularising the LLMs effect on selecting relevant information.
However, this ran into some inconsistent results, with only minor improvements.

The MLLM takes an image encoding and then projects it with an MLP to the LLM embedding space. We create a loss from the LLM token prediction loss. We add this loss to another, the cosine similarity between the projected image encoding and the projected image encoding projected again by another MLP projection.

<div align="center">
   <p>L<sub>cos</sub> = 1 - (P<sub>1</sub>(x) · P<sub>2</sub>(P<sub>1</sub>(x))) / (||P<sub>1</sub>(x)|| · ||P<sub>2</sub>(P<sub>1</sub>(x))||)</p>
</div>

The new approach avoids forcing the LLM space to match the vision space, which might have limited adaptation. Instead, it stabilizes the learned embeddings within the LLM's space, allowing them to be more structured and useful for token prediction. Which is critical for reasoning tasks like VQA.


# Regularising MLLM Alignment through Cosine Constrainment

<div align="center">
    <p>L<sub>Cos</sub> = 1 - (x · P<sub>1</sub>(x)) / (||x|| · ||P<sub>1</sub>(x)||)</p>
</div>

<div align="center">
    <p>L = L<sub>TP</sub> + &lambda;L<sub>Cos</sub></p>
</div>

<div align="center">
    <img src="Images/mistake/Cosine_Constrainment.PNG" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">
</div>

## Results


<iframe src="Images/html/original_embedding_by_regulisation_constant_2_last_step_unsorted.html" width="110%" height="700px" frameborder="0"></iframe>

<iframe src="Images/html/restored_projected_embedding_by_regulisation_constant_2_last_step_unsorted.html" width="110%" height="700px" frameborder="0"></iframe>


<div align="center">
    <img src="Images/W&B Chart 14_03_2025, 12_17_40.png" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">
</div>

# Regularising MLLM Alignment through Self-Distillation

<div align="center">
    <p>L<sub>Cos</sub> = 1 - (P<sub>1</sub>(x) · P<sub>2</sub>(P<sub>1</sub>(x))) / (||P<sub>1</sub>(x)|| · ||P<sub>2</sub>(P<sub>1</sub>(x))||)</p>
</div>

<div align="center">
    <p>L = L<sub>TP</sub> + &lambda;L<sub>Cos</sub></p>
</div>

<div align="center">
    <img src="Images/mistake/self-dsistillation.PNG" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">
</div>

- <p>P<sub>1</sub>(x) the image embeddings projected to the LLM embedding space</p>
- <p>P<sub>2</sub>(P<sub>1</sub>(x)) the image embeddings in the LLM embedding space regularised</p>
Self-Distillation for Visual-Language Alignment
The cosine similarity loss between two projections enhances MLLM performance by enforcing structural consistency in the embedding space:

<div align="center"> 
    <p>L<sub>cos</sub> = 1 - (P<sub>1</sub>(x) · P<sub>2</sub>(P<sub>1</sub>(x))) / (||P<sub>1</sub>(x)|| · ||P<sub>2</sub>(P<sub>1</sub>(x))||)</p>
</div>

Where:

P₁(x) projects image encodings into LLM space
P₂(P₁(x)) further transforms these projections
This approach provides several advantages:

- Structural Preservation: Forces projected embeddings to maintain consistent semantic relationships. When image features are projected, related concepts remain close in the embedding space, while unrelated concepts remain distant. This preserves the inherent visual relationships (e.g., "cat" and "dog" remain closer to each other than to "airplane") throughout the projection process. By maintaining these relationships, the LLM can better understand visual semantics when processing multimodal inputs.

- Regularization: Prevents overfitting and arbitrary mapping of features. The second projection P₂ acts as a consistency check on P₁, ensuring that the first projection learns robust, generalizable mappings rather than memorizing training examples. During training, this creates a supervision signal that guides P₁ to create more stable representations that maintain their semantic meanings even after further transformations, helping the model generalize to unseen images.

- Information Retention: Reduces information collapse when mapping between embedding spaces. Even though the LLM embedding space may be larger than the vision encoder's output space, without proper constraints, the projection could still map to a restricted subspace that loses important visual information. This approach ensures that semantically meaningful visual information is preserved and effectively utilized by the LLM.

- Modality Alignment: Makes visual features more compatible with text embeddings without forcing direct matching. Rather than imposing the vision encoder's representation structure onto the LLM space (which could limit the LLM's ability to reason with that information), this approach allows the visual information to adapt to the LLM's preferred representation format while maintaining internal consistency. This lets the LLM process visual data in a way that aligns with its pre-trained understanding of language.

This self-consistency mechanism creates more robust, information-preserving projections while allowing the model to adapt to the LLM's representation needs for reasoning tasks like VQA.

# Results

<div align="center">
    <img src="Images/mistake/W&B Chart 13_03_2025, 18_31_32.png" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">
</div>


## Other works

https://arxiv.org/pdf/2105.04906

https://arxiv.org/pdf/2407.04600v1

Other models preserve this consistency through attention mechanisms like flamingo using a gated cross-attention module to align visual tokens with LLM embeddings


## Next Steps

- Evaluate the information loss of the new model
- Keep looking for more data