# Week Progress

**Date**: 2025-03-03 to 2025-03-07

## Overview
- The LLM draws out the domain specific information with the MLP pprojection. This helps it to leverage its understanding and acheve a better accuracy.
- When we remove this by applying a cosine simularity to reconstructed embeddings after the projection, adding the loss to the original the accuracy decreases.
- This tells us that the MLP is not acting as a projector but also processing the inforamtion prior to the LLM.

- It also tells us that the LLM cannot handle the information on its own, requiring extra processing of the image embeddings before being passed to the LLM.
- This measn that the image embeddings have information that is not needed for the LLMs performace or information the LLM cannot process.

- It also tells us that the LLM is not able to unprocess information that is compressed? Requiring extra help to make the embeddings better:
  - Could try taking the embeddings and sppreading them across more embedding tokens, i.e increating the avalable information the proejcitons can provide. i.e one image embedding gets projected to two LLM embeddings?


- Issues:
  - could be because the new loss makes no diffrrence whenthe next token prediction loss is so large at the start. One of the issues we are trying to solve is the LLM infulences the projection very early, and cause loss of information it may benifit from later, i.e the projection is in a local mininum. However the loss of the cosine simularity and of the next token prediction loss are both decreased substantily, so this cannot be?


-  Other Ideas:

  - Due to the fact that the MLP clearly is benifitting the LLM in processing information.  We could try adding some larger more complex model as well as oor along side the orginal embeddings:
    - the orghinal would make the model generalisable and the extra proocessing of the emeddings would improve the MLLMs ability?

  - Could also try MoE on the embeddings


  Quesitions?

  - Do Qformers have this same issue? And is the MLP acting as a bad qformer, i.e selecting information that is helpfull to the m,ost likly query?
  - Can we improve this like the QFormers do but without using the query?


## Regulerisation

<img src="Images/reg_const.PNG" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">

## Loss

- As the reg parameter increases the loss also increases. This means that the cosine simularity could not be being otimised as well, in which case the dimentionality reduction is resulting in most of the information loss.
- It sould indicate that as the projection becomes better at reconstructing the orginal embeddings that the projection is no longer able to effectivilly preporcess the embeddings for the LLM. This is less likly as we see the accuracy increasing with the reg parameter, even if it is only slight.



## Issues

- The cosine simularity does not take magnitude into acount and only calulates the angles between the embedings. Embeddings need to be normilised as to not effect the gradient?




## Accomplishments

## Plans for Next Week
