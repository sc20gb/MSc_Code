# Week Progress

**Date**: 2025-02-03 to 2025-02-07

## Overview

- DeepSeek was explored further
- Medical papers from the literature review from week 2025-01-25 have been explored further to:
    - Assess if they use a pre-trained vision encoder and LLM connected through a type of projection module
    - And if they have to see how they performed their embedding alignment

## DeepSeek-V3

<img src="Images/DeepSeekPNG.PNG" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">


- Mixture-of-Experts (MoE): 671B total parameters, but only 37B active per token.

- Expert Selection: Uses 8 out of 256 experts plus shared experts.

- Gating Network: Dynamically routes inputs to the most relevant experts:

<img src="Images/MoEeq.PNG" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">

- Multi-Head Latent Attention (MLA): Reduces memory usage while maintaining efficiency

## Embedding Alignment in Medical MLLMs

- No papers found leveraged non-medical data to further alignment tuning

- Of the 25 medical papers, 8 were considered irrelevant for the following reasons:

    - 1: for training a transformer for VQA as classification not language generation
    - 2: for leveraging pre-made models without training them, integrating them into systems to explore real-world use
    - 1: for evaluating GPTs
    - 2: for not using Large Language Models (LLMs)
    - 1: used cross-modal reasoning as a classifier
    - 1: for being a review on the practicalities of using MLLMs in dentistry.

- Of the remaining 17:

**With Explicit Alignment Training**
### CXR-LLaVA*
Architecture:
    Visual: ViT-L/16
    LLM: Llama-2 7B
    Connection: MLP projection
Alignment Strategy:
        Phase 1: Train projection only
        Phase 2: LoRA + projection training


<img src="Images/CXR-LLaVA.png" alt="Image 1" style="flex: 1; max-width: 100%; height: auto;">



### PLAMi
Architecture:
    Visual: BioMEDCLIP
    LLM: SevMistral-7B
    Connection: MLP projection
Alignment Strategy:
    PMC-15M biomedical concept alignment
    Frozen visual encoder
    Trainable projector and feature extractor
### OphGLM
Architecture:
    Visual: ConvNext
    LLM: BERT + ChatGLM-6B
    Connection: 6-layer fusion module
Alignment Strategy:
    Frozen encoders and LLM
    Only fusion module trained


**Without Explicit Alignment:**

### SkinGPT-4
Architecture:
    Visual: ViT
    LLM: Llama-2-13b-chat
    Connection: Direct projection
Why No Alignment:
    Large dataset approach
    End-to-end training
    Relies on natural convergence
### Vision-Language Foundation Model
    Architecture:
    Visual: MUSK transformer
    LLM: Not specified
    Connection: Direct concatenation
Why No Alignment:
    Uses unified masked modeling
    Joint representation learning
    No explicit feature space bridging
### IQAGPT
Architecture:
    Visual: ViT-S/16
    LLM: BERT (first 6 layers)
    Connection: Cross-attention
Why No Alignment:
    Small model size
    Direct training feasible
    Simpler architecture
### ClinicalBLIP
Architecture:
    Visual: ViT encoder
    LLM: Q-former -> FFN -> LLM
    Connection: Q-former
Why No Alignment:
    End-to-end training
    Q-former handles bridging
    No explicit alignment phase





## Accomplishments

- Identified methods used for embedding alignment in medical MLLMs
- Reviewed the DeepSeek architecture in detail and researched the reinforcement learning method used to produce DeepSeek-R1-Zero,
    to explore the potential of using R1-Distilled models in MLLM alignment tuning and MLLM instruction tuning