# Week Progress

**Date**: 2024-11-18 to 2024-11-22

## Overview

Due to the computational requirements of using Vicuna-13b, a smaller model, TinyLama, was implemented. The smaller model should allow for better hyperpameters for the Low-Rank-Adaptation of the language model that do not cause instability.

However, the smaller model was incompatible with the existing autoregressive generation function. To preserve the compatibility of future LamaModels, selective parts of the transformers library have been overwritten (see the utils folder), allowing the original generation function to be used.

The TinyLama paper can be found [here](https://arxiv.org/abs/2401.02385).


Files modified:

[fine tuning script](https://github.com/sc20gb/MSc_Code/blob/main/fine_tune_with_gen.py)

[utility files](https://github.com/sc20gb/MSc_Code/tree/main/utils)

[MLLM class](https://github.com/sc20gb/MSc_Code/blob/main/Model_Defs%2Fconnector_LLM_with_gen.py)

## Accomplishments

- Implemented a more efficient MLLM class by modifying the transformers library.
- Created more efficient versions to fine tune the new MLLM class.
- Tested the MLLM class to be compatible with all models used so far: Vicuna-13b, Vicuna-7b, and TinyLama.

## Plans for Next Week

- Get some results/ a working model with the new MLLM model

- Catchup on paper reading